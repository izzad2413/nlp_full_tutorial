{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOk4+BtdyeNm70OLyhE0/Pi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"hC2KITPItkpK"},"outputs":[],"source":["# machine learning for nlp\n","# nltk https://www.nltk.org/\n","# spacy https://spacy.io/"]},{"cell_type":"code","source":["!pip install nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aLPYIL3VuWIU","executionInfo":{"status":"ok","timestamp":1723736921469,"user_tz":-480,"elapsed":4480,"user":{"displayName":"Nazmirul Izzad Nassir","userId":"07516674803625853227"}},"outputId":"eaa77d88-efe9-48f2-b3f1-82dbaa8fd755"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X-ntmtauvbMD","executionInfo":{"status":"ok","timestamp":1723737187692,"user_tz":-480,"elapsed":1065,"user":{"displayName":"Nazmirul Izzad Nassir","userId":"07516674803625853227"}},"outputId":"8ba03bc2-238a-4543-d2fb-a05aaefa1c35"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["corpus = \"\"\"Hello welcome, to everyone NLP Tutorials.\n","Please watch till the end! to become expert in NLP. It's fun.\n","\"\"\""],"metadata":{"id":"3Ep6cqj-ujJ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yg5uxlCkuokC","executionInfo":{"status":"ok","timestamp":1723737743805,"user_tz":-480,"elapsed":401,"user":{"displayName":"Nazmirul Izzad Nassir","userId":"07516674803625853227"}},"outputId":"3847e045-94af-4423-afee-c2ef76aadb88"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello welcome, to everyone NLP Tutorials. \n","Please watch till the end! to become expert in NLP. It's fun.\n","\n"]}]},{"cell_type":"markdown","source":["## 1.0 The Process of Tokenization"],"metadata":{"id":"HKeZ-UVkvlC7"}},{"cell_type":"code","source":["# convert paragraph --> sentences (as a list)\n","# general use case\n","from nltk.tokenize import sent_tokenize"],"metadata":{"id":"URC7TmrMuork"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["documents = sent_tokenize(corpus)"],"metadata":{"id":"RVuZP9x0vTNz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["documents"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WHCfc3w7wL_O","executionInfo":{"status":"ok","timestamp":1723737751682,"user_tz":-480,"elapsed":3,"user":{"displayName":"Nazmirul Izzad Nassir","userId":"07516674803625853227"}},"outputId":"e77acc2d-a4e3-484b-e082-95c8172b093e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hello welcome, to everyone NLP Tutorials.',\n"," 'Please watch till the end!',\n"," 'to become expert in NLP.',\n"," \"It's fun.\"]"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["type(documents)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GGd8nqFgwOfW","executionInfo":{"status":"ok","timestamp":1723737770453,"user_tz":-480,"elapsed":421,"user":{"displayName":"Nazmirul Izzad Nassir","userId":"07516674803625853227"}},"outputId":"3c705000-e375-4019-e6c4-a31f4434433c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["list"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["for sentence in documents:\n","  print(sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"39DyFsuZwFYO","executionInfo":{"status":"ok","timestamp":1723737772869,"user_tz":-480,"elapsed":397,"user":{"displayName":"Nazmirul Izzad Nassir","userId":"07516674803625853227"}},"outputId":"2b37371f-8505-4406-c1f6-5b3450d95e2b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello welcome, to everyone NLP Tutorials.\n","Please watch till the end!\n","to become expert in NLP.\n","It's fun.\n"]}]},{"cell_type":"markdown","source":["## 2.0 The Process of Tokenization"],"metadata":{"id":"XbRprk1dwauI"}},{"cell_type":"code","source":["# paragraph --> words\n","# sentence --> words\n","# split by word\n","# general use case\n","from nltk.tokenize import word_tokenize"],"metadata":{"id":"KdtVnwcnwQ7P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["word_tokenize(corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xoJeqlU9wQ_R","executionInfo":{"status":"ok","timestamp":1723737780768,"user_tz":-480,"elapsed":405,"user":{"displayName":"Nazmirul Izzad Nassir","userId":"07516674803625853227"}},"outputId":"5737fae8-20e4-4f81-ba93-ac8d9a9e38b4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hello',\n"," 'welcome',\n"," ',',\n"," 'to',\n"," 'everyone',\n"," 'NLP',\n"," 'Tutorials',\n"," '.',\n"," 'Please',\n"," 'watch',\n"," 'till',\n"," 'the',\n"," 'end',\n"," '!',\n"," 'to',\n"," 'become',\n"," 'expert',\n"," 'in',\n"," 'NLP',\n"," '.',\n"," 'It',\n"," \"'s\",\n"," 'fun',\n"," '.']"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["for sentence in documents:\n","  print(word_tokenize(sentence))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fFixi6SPwRBq","executionInfo":{"status":"ok","timestamp":1723737788040,"user_tz":-480,"elapsed":392,"user":{"displayName":"Nazmirul Izzad Nassir","userId":"07516674803625853227"}},"outputId":"d371e96b-5467-4272-b0e8-a496160ea951"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', 'welcome', ',', 'to', 'everyone', 'NLP', 'Tutorials', '.']\n","['Please', 'watch', 'till', 'the', 'end', '!']\n","['to', 'become', 'expert', 'in', 'NLP', '.']\n","['It', \"'s\", 'fun', '.']\n"]}]},{"cell_type":"code","source":["from nltk.tokenize import wordpunct_tokenize"],"metadata":{"id":"4dRNKiJ8wRD4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wordpunct_tokenize(corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OprU3fwyxbsP","executionInfo":{"status":"ok","timestamp":1723737796589,"user_tz":-480,"elapsed":4,"user":{"displayName":"Nazmirul Izzad Nassir","userId":"07516674803625853227"}},"outputId":"c642f70e-d554-4959-da42-870c97cc3349"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hello',\n"," 'welcome',\n"," ',',\n"," 'to',\n"," 'everyone',\n"," 'NLP',\n"," 'Tutorials',\n"," '.',\n"," 'Please',\n"," 'watch',\n"," 'till',\n"," 'the',\n"," 'end',\n"," '!',\n"," 'to',\n"," 'become',\n"," 'expert',\n"," 'in',\n"," 'NLP',\n"," '.',\n"," 'It',\n"," \"'\",\n"," 's',\n"," 'fun',\n"," '.']"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","source":["## 3.0 The Process of Tokenization"],"metadata":{"id":"jvzEkpdCyV8i"}},{"cell_type":"code","source":["from nltk.tokenize import TreebankWordTokenizer\n","# fullstop will be included in the previous word\n","# fullstop last word will be separate"],"metadata":{"id":"0bN4onD1xbY7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = TreebankWordTokenizer()"],"metadata":{"id":"4T6s06z5xbbS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.tokenize(corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QWKy11TixbdZ","executionInfo":{"status":"ok","timestamp":1723737890171,"user_tz":-480,"elapsed":446,"user":{"displayName":"Nazmirul Izzad Nassir","userId":"07516674803625853227"}},"outputId":"6a6a52e7-ae97-4788-afee-f5a2a4575e3f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hello',\n"," 'welcome',\n"," ',',\n"," 'to',\n"," 'everyone',\n"," 'NLP',\n"," 'Tutorials.',\n"," 'Please',\n"," 'watch',\n"," 'till',\n"," 'the',\n"," 'end',\n"," '!',\n"," 'to',\n"," 'become',\n"," 'expert',\n"," 'in',\n"," 'NLP.',\n"," 'It',\n"," \"'s\",\n"," 'fun',\n"," '.']"]},"metadata":{},"execution_count":35}]}]}